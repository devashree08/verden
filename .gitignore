#!/usr/bin/env python3
"""
SQL Server -> BigQuery ETL (full replace), with atlas descriptions.

- Keeps source column names exactly as-is (including spaces).
- Always applies atlas column descriptions by matching atlas names to BQ columns
  using a normalized key (strip all non-alphanumerics), so:
    "Tier 2 Manager"  <->  "Tier2Manager"
- No partitioning, no date coercion, no optional behavior.

Required env:
  DB_SERVER, DB_DATABASE, DB_USERNAME, DB_PASSWORD   # MSSQL
  BQ_PROJECT, BQ_DATASET                             # BigQuery target
Optional env:
  BQ_LOCATION (default "US")

CLI (all required):
  --sql "<SELECT or EXEC ...>"
  --dest-table <table_name_in_bq>
  --atlas <path/to/*.atlas.yml>
"""

from __future__ import annotations

import argparse
import os
import sys
import time
import re
from typing import Iterable, List, Dict

import pandas as pd
import pyodbc
from google.cloud import bigquery

try:
    import yaml
except Exception as e:
    print("ERROR: pyyaml is required for atlas processing. pip install pyyaml", file=sys.stderr)
    raise

try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

# ---------- helpers ----------

def env_required(name: str) -> str:
    val = os.getenv(name)
    if not val:
        print(f"ERROR: missing required env var {name}", file=sys.stderr)
        sys.exit(2)
    return val

def connect_mssql(
    server: str, database: str, username: str, password: str,
    encrypt: bool = False, trust_server_cert: bool = True, timeout_sec: int = 5
) -> pyodbc.Connection:
    driver = "{ODBC Driver 18 for SQL Server}"
    enc = "yes" if encrypt else "no"
    tsc = "yes" if trust_server_cert else "no"
    conn_str = (
        f"DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password};"
        f"Encrypt={enc};TrustServerCertificate={tsc};Connection Timeout={timeout_sec};"
    )
    return pyodbc.connect(conn_str)

def iter_df_chunks(cursor: pyodbc.Cursor, sql: str, arraysize: int = 20000) -> Iterable[pd.DataFrame]:
    cursor.execute(sql)
    cols = [c[0] for c in cursor.description]
    while True:
        rows = cursor.fetchmany(arraysize)
        if not rows:
            break
        yield pd.DataFrame.from_records(rows, columns=cols)

def infer_bq_schema(df: pd.DataFrame) -> List[bigquery.SchemaField]:
    fields: List[bigquery.SchemaField] = []
    for name, dtype in df.dtypes.items():
        if pd.api.types.is_integer_dtype(dtype):
            kind = "INT64"
        elif pd.api.types.is_float_dtype(dtype):
            kind = "FLOAT64"
        elif pd.api.types.is_bool_dtype(dtype):
            kind = "BOOL"
        elif pd.api.types.is_datetime64_any_dtype(dtype):
            kind = "DATETIME"
        else:
            kind = "STRING"
        fields.append(bigquery.SchemaField(name, kind))
    return fields

_norm = re.compile(r"[^0-9A-Za-z]+")

def norm_key(name: str) -> str:
    return _norm.sub("", name or "")

def apply_atlas_descriptions(client: bigquery.Client, full_table: str, atlas_path: str) -> None:
    # load atlas
    if not os.path.exists(atlas_path):
        print(f"ERROR: atlas file not found: {atlas_path}", file=sys.stderr)
        sys.exit(2)
    atlas = yaml.safe_load(open(atlas_path, "r", encoding="utf-8")) or {}
    cols = atlas.get("columns") or []
    # atlas names are normalized (e.g., Tier2Manager). Build map by normalized key.
    atlas_by_norm: Dict[str, str] = {}
    for c in cols:
        if not isinstance(c, dict):
            continue
        name = (c.get("name") or "").strip()
        if not name:
            continue
        desc = (c.get("description") or "").strip()
        atlas_by_norm[norm_key(name)] = desc

    # fetch BQ table and rewrite descriptions by normalized match
    table = client.get_table(full_table)
    new_schema: List[bigquery.SchemaField] = []
    changed = 0
    for f in table.schema:
        key = norm_key(f.name)
        new_desc = atlas_by_norm.get(key, f.description)
        if (new_desc or "") != (f.description or ""):
            changed += 1
        new_schema.append(bigquery.SchemaField(f.name, f.field_type, mode=f.mode, description=new_desc))

    if changed:
        table.schema = new_schema
        client.update_table(table, ["schema"])
        print(f"Applied {changed} atlas descriptions to {full_table}", file=sys.stderr)
    else:
        print(f"No description changes needed on {full_table}", file=sys.stderr)

# ---------- main ----------

def main() -> int:
    ap = argparse.ArgumentParser("MSSQL -> BigQuery ETL (full replace) with atlas descriptions.")
    ap.add_argument("--sql", required=True, help='SQL to run on MSSQL (e.g., "SELECT ..." or "EXEC dbo.SP...")')
    ap.add_argument("--dest-table", required=True, help="BigQuery table id (e.g., all_apps_values)")
    ap.add_argument("--atlas", required=True, help="Path to atlas *.atlas.yml")
    args = ap.parse_args()

    # env
    server = env_required("DB_SERVER")
    database = env_required("DB_DATABASE")
    username = env_required("DB_USERNAME")
    password = env_required("DB_PASSWORD")
    encrypt = os.getenv("DB_ENCRYPT", "false").lower() == "true"
    trust = os.getenv("DB_TRUST_SERVER_CERT", "true").lower() == "true"
    conn_timeout = int(os.getenv("DB_CONN_TIMEOUT_SEC", "5"))

    project = env_required("BQ_PROJECT")
    dataset = env_required("BQ_DATASET")
    location = os.getenv("BQ_LOCATION", "US")
    full_table = f"{project}.{dataset}.{args.dest_table}"

    t0 = time.time()
    print(f"ETL start: MSSQL -> {full_table}", file=sys.stderr)

    # read from MSSQL
    with connect_mssql(server, database, username, password, encrypt, trust, conn_timeout) as conn:
        cur = conn.cursor()
        dfs: List[pd.DataFrame] = []
        total = 0
        for i, chunk in enumerate(iter_df_chunks(cur, args.sql, arraysize=20000), start=1):
            total += len(chunk.index)
            dfs.append(chunk)
            print(f" - fetched chunk {i}: {len(chunk.index)} rows", file=sys.stderr)
        df = pd.DataFrame() if not dfs else pd.concat(dfs, ignore_index=True)
        print(f"Fetched rows: {total}", file=sys.stderr)

    # bigquery client & ensure dataset
    bq = bigquery.Client(project=project)
    ds_ref = bigquery.Dataset(f"{project}.{dataset}")
    try:
        bq.get_dataset(ds_ref)
    except Exception:
        ds_ref.location = location
        bq.create_dataset(ds_ref, exists_ok=True)
        print(f"Dataset ensured: {project}.{dataset}", file=sys.stderr)

    # schema + load (truncate)
    schema = infer_bq_schema(df)
    job_config = bigquery.LoadJobConfig(
        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
        schema=schema,
    )
    job = bq.load_table_from_dataframe(df, full_table, job_config=job_config)
    job.result()

    table = bq.get_table(full_table)
    print(f"Loaded {table.num_rows} rows into {full_table} (fields={len(table.schema)}) in {round(time.time()-t0,2)}s",
          file=sys.stderr)

    # always apply atlas descriptions
    apply_atlas_descriptions(bq, full_table, args.atlas)

    print("ETL complete.", file=sys.stderr)
    return 0

if __name__ == "__main__":
    raise SystemExit(main())
