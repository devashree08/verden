from __future__ import annotations

import argparse
import datetime as dt
import re
from pathlib import Path
from typing import Any, Dict, List, Optional

import yaml

REG_DIR = Path("registry")
ATLAS_DIR = Path("atlas")
GLOSSARY_FILE = Path("ontology/glossary.yml")


# ---------- IO helpers ----------

def load_registry() -> Dict[str, Dict[str, Any]]:
    out: Dict[str, Dict[str, Any]] = {}
    for p in REG_DIR.glob("*.yml"):
        data = yaml.safe_load(p.read_text(encoding="utf-8"))
        # expect keys: procedure, result_columns
        if "procedure" in data and "result_columns" in data:
            out[data["procedure"]] = data
    return out


def load_existing_atlas(proc: str) -> Dict[str, Dict[str, Any]]:
    """Return {column_name: atlas_entry} if an atlas file exists, else {}."""
    p = ATLAS_PATH(proc)
    if not p.exists():
        return {}
    doc = yaml.safe_load(p.read_text(encoding="utf-8")) or {}
    cols = doc.get("columns", []) or []
    by_name = {c["name"]: c for c in cols if isinstance(c, dict) and "name" in c}
    return by_name


def ATLAS_PATH(proc: str) -> Path:
    # filename is "<schema.proc>.atlas.yml" with dots kept (easy to eyeball)
    return ATLAS_DIR / f"{proc}.atlas.yml"


def load_glossary() -> Dict[str, str]:
    if not GLOSSARY_FILE.exists():
        return {}
    data = yaml.safe_load(GLOSSARY_FILE.read_text(encoding="utf-8")) or {}
    # expected format: { term: "expansion" }
    return {str(k).lower(): str(v) for k, v in data.items()}


def write_atlas(proc: str, merged_columns: List[Dict[str, Any]], force: bool) -> bool:
    """
    Write atlas file if changed or force=True. Returns True if written.
    """
    ATLAS_DIR.mkdir(parents=True, exist_ok=True)
    p = ATLAS_PATH(proc)
    payload = {
        "procedure": proc,
        "generated_at": dt.datetime.utcnow().isoformat(timespec="seconds") + "Z",
        "columns": merged_columns,
    }
    new_text = yaml.safe_dump(payload, sort_keys=False, allow_unicode=True, width=120)

    if p.exists() and not force:
        old_text = p.read_text(encoding="utf-8")
        if old_text == new_text:
            return False

    p.write_text(new_text, encoding="utf-8")
    return True


# ---------- Name parsing & templates ----------

_CAMEL_BOUNDARY = re.compile(r"(?<=[a-z0-9])(?=[A-Z])")
_NON_ALNUM = re.compile(r"[^A-Za-z0-9]+")

def tokenize(name: str) -> List[str]:
    """
    Split a column name into lowercase tokens from snake/camel/mixed cases.
    Examples:
      "ReportMonth" -> ["report","month"]
      "isPCICompliant" -> ["is","pci","compliant"]
      "critical_vuln_count" -> ["critical","vuln","count"]
    """
    # split on non-alnum first, then camel boundaries
    parts: List[str] = []
    for chunk in _NON_ALNUM.split(name):
        if not chunk:
            continue
        parts.extend(_CAMEL_BOUNDARY.split(chunk))
    return [p.lower() for p in parts if p]


def gloss(tokens: List[str], glossary: Dict[str, str]) -> List[str]:
    """
    Expand known abbreviations using optional glossary.
    """
    if not glossary:
        return tokens
    out: List[str] = []
    for t in tokens:
        out.extend(glossary.get(t, t).split())
    return out


SEVERITY = {"critical", "high", "medium", "low"}
VULN_HINTS = {"vuln", "vulns", "vulnerability", "vulnerabilities"}
COUNT_HINTS = {"count", "num", "total"}
PCT_HINTS = {"pct", "percent", "percentage"}
BOOL_HINTS_PREFIX = {"is", "has"}
FLAG_HINTS_SUFFIX = {"flag"}
DATE_HINTS = {"date", "dt", "datetime", "timestamp", "month", "reportmonth", "reported", "asof", "as_at"}
ID_SUFFIX = {"id", "identifier"}

ORG_HINTS = {"manager", "owner", "custodian", "bu", "businessunit", "team", "group"}
COMPLIANCE_HINTS = {"sox", "pci", "pcitype", "garm", "cpi306", "cpni", "hardwarestatus", "status", "compliant"}

def join_phrase(words: List[str]) -> str:
    return " ".join(words)


def template_description(col_name: str, sql_type: str, tokens: List[str]) -> Dict[str, Any]:
    """
    Return {"description": str, "tags": [..], "needs_review": bool}
    Deterministic; no LLM calls.
    """
    tset = set(tokens)
    tags: List[str] = []
    needs_review = False

    # 1) Strong identities
    if set(tokens) == {"report", "month"} or "reportmonth" in tset:
        return {
            "description": "Snapshot month for these metrics (YYYY-MM-01 00:00:00).",
            "tags": ["time"],
            "needs_review": False,
        }
    if col_name.lower() in {"vast", "vastid", "vast_id"} or (len(tokens) == 2 and tokens[1] in ID_SUFFIX and tokens[0] == "vast"):
        return {
            "description": "Unique application identifier (VAST).",
            "tags": ["identity"],
            "needs_review": False,
        }

    # 2) Flags / booleans
    if tokens and (tokens[0] in BOOL_HINTS_PREFIX or tokens[-1] in FLAG_HINTS_SUFFIX):
        tags.extend(["flag"])
        base = [w for w in tokens if w not in BOOL_HINTS_PREFIX and w not in FLAG_HINTS_SUFFIX]
        noun = join_phrase(base) if base else col_name
        return {
            "description": f"Whether the VAST is {noun} (1 = yes, 0 = no).",
            "tags": tags,
            "needs_review": False,
        }

    # 3) Percentages
    if PCT_HINTS & tset or sql_type.upper().startswith(("DECIMAL", "NUMERIC")) and any(w in tokens for w in ("pct", "percent", "percentage")):
        base = [w for w in tokens if w not in PCT_HINTS]
        noun = join_phrase(base) or col_name
        return {
            "description": f"Percentage of {noun} (0â€“100).",
            "tags": ["percentage"],
            "needs_review": False,
        }

    # 4) Counts (including vuln counts)
    if (COUNT_HINTS & tset) or any(w.endswith("count") for w in tokens):
        tags.append("count")
        # If it looks like a severity vuln count, describe it nicely
        sev = list(SEVERITY & tset)
        if sev and (VULN_HINTS & tset):
            sev_str = sev[0]
            base = [w for w in tokens if w not in COUNT_HINTS | SEVERITY | VULN_HINTS]
            scope = join_phrase(base) or "the application"
            return {
                "description": f"Number of {sev_str} vulnerabilities ({scope}).",
                "tags": tags + ["vulnerability"],
                "needs_review": False,
            }
        # generic count
        base = [w for w in tokens if w not in COUNT_HINTS]
        noun = join_phrase(base) or col_name
        return {
            "description": f"Count of {noun}.",
            "tags": tags,
            "needs_review": False,
        }

    # 5) Vulnerability metrics without count
    if VULN_HINTS & tset:
        sev = list(SEVERITY & tset)
        sev_str = sev[0] + " " if sev else ""
        base = [w for w in tokens if w not in VULN_HINTS | SEVERITY]
        scope = join_phrase(base)
        desc = f"{sev_str}vulnerability metric"
        if scope:
            desc += f" ({scope})"
        return {"description": desc + ".", "tags": ["vulnerability"], "needs_review": True}

    # 6) Dates/timestamps
    if DATE_HINTS & tset or sql_type.upper() in {"DATE", "DATETIME", "SMALLDATETIME", "DATETIME2"}:
        base = [w for w in tokens if w not in DATE_HINTS]
        noun = join_phrase(base) or "record"
        return {
            "description": f"Date/time associated with {noun}.",
            "tags": ["time"],
            "needs_review": False,
        }

    # 7) Org / compliance hints
    if ORG_HINTS & tset:
        return {
            "description": f"Organizational attribute ({join_phrase(tokens)}).",
            "tags": ["org"],
            "needs_review": True,
        }
    if COMPLIANCE_HINTS & tset:
        return {
            "description": f"Compliance-related attribute ({join_phrase(tokens)}).",
            "tags": ["compliance"],
            "needs_review": True,
        }

    # 8) IDs
    if tokens and tokens[-1] in ID_SUFFIX:
        base = tokens[:-1] or ["entity"]
        return {
            "description": f"Identifier for {' '.join(base)}.",
            "tags": ["identity"],
            "needs_review": False,
        }

    # 9) Fallback
    return {
        "description": f"Metric for {col_name} (details TBD).",
        "tags": [],
        "needs_review": True,
    }


def merge_columns(
    proc: str,
    registry_cols: List[Dict[str, Any]],
    existing_by_name: Dict[str, Dict[str, Any]],
    glossary: Dict[str, str],
) -> List[Dict[str, Any]]:
    """
    Produce the atlas columns array in registry order, preserving existing descriptions/tags/etc.
    Only fill blanks; never overwrite non-empty description on re-run.
    """
    merged: List[Dict[str, Any]] = []
    for c in registry_cols:
        name = c["name"]
        sql_type = c.get("sql_type", "")
        existing = existing_by_name.get(name, {})  # may be empty

        # preserve existing description if present and non-empty
        desc = (existing.get("description") or "").strip()

        # compute defaults if missing
        if not desc:
            tokens = gloss(tokenize(name), glossary)
            gen = template_description(name, sql_type, tokens)
            desc = gen["description"]
            gen_tags = gen.get("tags", [])
            needs_review = bool(gen.get("needs_review", False))
        else:
            gen_tags = []
            needs_review = bool(existing.get("needs_review", False))

        # tags: union existing + generated (unique, preserve order)
        tags_existing = existing.get("tags", []) or []
        tags = list(dict.fromkeys([*tags_existing, *gen_tags]))

        # carry over any OpenAPI-ish hints if present
        out = {
            "name": name,
            "description": desc,
            "tags": tags,
            "needs_review": needs_review,
        }
        # keep any optional keys the user may have added
        for k in ("format", "pattern", "example", "deprecated", "x-needs-review"):
            if k in existing and existing[k] is not None:
                out[k] = existing[k]

        merged.append(out)
    return merged


# ---------- CLI ----------

def main() -> int:
    parser = argparse.ArgumentParser(
        prog="atlas-generate",
        description="Generate/refresh column descriptions in atlas/*.atlas.yml from registry/*.yml using deterministic templates.",
    )
    parser.add_argument("--proc", action="append", help="Limit to one or more procedures (e.g., dbo.SPGetAllAppsValueByEID).")
    parser.add_argument("--dry-run", action="store_true", help="Do not write files; just report planned changes.")
    parser.add_argument("--force", action="store_true", help="Write even if content is unchanged (updates generated_at).")
    args = parser.parse_args()

    registry = load_registry()
    if not registry:
        print("No registry/*.yml found. Run your Phase-1 generator first.")
        return 2

    glossary = load_glossary()

    targets = args.proc or sorted(registry.keys())
    wrote_any = False
    for proc in targets:
        entry = registry.get(proc)
        if not entry:
            print(f"[skip] Unknown procedure '{proc}'")
            continue
        existing = load_existing_atlas(proc)
        merged = merge_columns(proc, entry["result_columns"], existing, glossary)

        if args.dry_run:
            # Report how many were filled
            total = len(merged)
            newly_described = sum(
                1
                for m in merged
                if m.get("description") and not (existing.get(m["name"], {}).get("description"))
            )
            needs_review = sum(1 for m in merged if bool(m.get("needs_review")))
            print(f"[dry-run] {proc}: {newly_described}/{total} descriptions filled, {needs_review} need review")
        else:
            changed = write_atlas(proc, merged, force=args.force)
            wrote_any = wrote_any or changed
            status = "wrote" if changed else "unchanged"
            needs_review = sum(1 for m in merged if bool(m.get("needs_review")))
            print(f"[{status}] {proc}: {len(merged)} columns, {needs_review} need review")

    return 0 if (wrote_any or args.dry_run) else 0


if __name__ == "__main__":
    raise SystemExit(main())
