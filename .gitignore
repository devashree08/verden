#!/usr/bin/env python
"""
Generate/refresh field dictionaries for MCP resources.

- Introspects MSSQL stored procs and/or BigQuery tables to discover columns.
- Produces/updates YAML files under server/resources/fields/.
- Fills normalized types, physical types (mssql_type/bq_type), overlap metadata,
  canonical_source, and placeholder descriptions (or AI-assisted if enabled).

Usage (dry run):
    uv run python scripts/gen_field_dicts.py --from mssql --from bigquery --email alice@alice.com

Persist changes:
    uv run python scripts/gen_field_dicts.py --from mssql --from bigquery --email alice@alice.com --write

Enable Gemini for descriptions (OPTIONAL, needs local CLI + approval):
    uv run python scripts/gen_field_dicts.py --from mssql --from bigquery --email alice@alice.com --write --ai --model gemini-1.5-pro

Notes:
- We never send real row values to AI. Prompts include only column names, inferred types,
  table purpose, and synthetic example values.
///

Repo assumptions:
- MSSQL procs:
  dbo.SPGetAllAppsValueByUser(@Email, @VAST=NULL)
  dbo.SPGetAllAppsSummaryByUser(@Email, @VAST=NULL, @ReportMonth=NULL)
  dbo.SPGetVastGeneralByUser(@Email, @VAST=NULL)

- BigQuery tables in env:
  BQ_DATASET + table names:
    BQ_TABLE_ALLAPPS_VALUE
    BQ_TABLE_ALLAPPS_SUMMARY
    BQ_TABLE_VAST_GENERAL
"""

from __future__ import annotations

import argparse
import datetime as dt
import json
import os
import subprocess
import sys
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

# Optional imports guarded later
# - pyodbc (MSSQL)
# - google.cloud.bigquery (BQ)
from ruamel.yaml import YAML

# ----- Config / constants -----

ROOT = Path(__file__).resolve().parents[1]
FIELDS_DIR = ROOT / "server" / "resources" / "fields"

# Tool <-> file mapping (keep in sync with describe_fields.py)
TOOL_TO_FILE = {
    "get_all_apps_value_by_user": FIELDS_DIR / "allapps_value.yaml",
    "get_all_apps_summary_by_user": FIELDS_DIR / "allapps_summary.yaml",
    "get_vast_general_by_user": FIELDS_DIR / "vast_general.yaml",
}

# Canonical source rules (simple + predictable)
CANONICAL_BY_TOOL = {
    "get_all_apps_value_by_user": "current",           # daily, current-month
    "get_all_apps_summary_by_user": "monthly_snapshot",# frozen monthly
    "get_vast_general_by_user": "current",             # general/compliance
}

# ----- Helpers: YAML IO -----

yaml_rt = YAML()
yaml_rt.default_flow_style = False
yaml_rt.indent(sequence=2, offset=2)

def _load_yaml_or_seed(path: Path, tool_name: str) -> Dict[str, Any]:
    if not path.exists():
        return {
            "schema_version": 1,
            "tool": tool_name,
            "temporal_scope": CANONICAL_BY_TOOL[tool_name],
            "last_updated": dt.date.today().isoformat(),
            "notes": "",
            "fields": [],
        }
    y = YAML(typ="safe")
    with path.open("r", encoding="utf-8") as f:
        data = y.load(f) or {}
    # sanity
    data.setdefault("schema_version", 1)
    data.setdefault("tool", tool_name)
    data.setdefault("temporal_scope", CANONICAL_BY_TOOL[tool_name])
    data.setdefault("last_updated", dt.date.today().isoformat())
    data.setdefault("notes", "")
    data.setdefault("fields", [])
    return data

def _write_yaml(path: Path, data: Dict[str, Any]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    data["last_updated"] = dt.date.today().isoformat()
    with path.open("w", encoding="utf-8") as f:
        yaml_rt.dump(data, f)

# ----- Type inference -----

def normalize_type_from_value(py_val: Any) -> str:
    """Map a Python value to our normalized type."""
    import decimal
    import datetime
    if py_val is None:
        # Unknown -> favor string; will be refined by schema types if available.
        return "string"
    if isinstance(py_val, bool):
        return "bool"
    if isinstance(py_val, int) and not isinstance(py_val, bool):
        return "int"
    if isinstance(py_val, (float, decimal.Decimal)):
        return "decimal"
    if isinstance(py_val, (datetime.date, datetime.datetime)):
        return "datetime"
    return "string"

def mssql_type_from_value(py_val: Any) -> str:
    """Best-effort MSSQL type from a Python value."""
    import decimal
    import datetime
    if py_val is None:
        return "varchar"
    if isinstance(py_val, bool):
        return "bit"
    if isinstance(py_val, int) and not isinstance(py_val, bool):
        return "int"
    if isinstance(py_val, (float, decimal.Decimal)):
        return "decimal"
    if isinstance(py_val, (datetime.date, datetime.datetime)):
        return "datetime"
    # bytes? treat as varbinary; else varchar
    if isinstance(py_val, (bytes, bytearray)):
        return "varbinary"
    return "varchar"

def bq_type_from_normalized(norm: str) -> str:
    """Map our normalized type to a BigQuery type."""
    return {
        "string": "STRING",
        "int": "INT64",
        "decimal": "NUMERIC",
        "datetime": "DATETIME",
        "bool": "BOOL",
    }.get(norm, "STRING")

# ----- MSSQL introspection -----

def mssql_connect() -> Any:
    import pyodbc
    host = os.getenv("SQLSERVER_HOST", "")
    port = os.getenv("SQLSERVER_PORT", "1433")
    db = os.getenv("SQLSERVER_DATABASE", "")
    user = os.getenv("SQLSERVER_USERNAME", "")
    pwd = os.getenv("SQLSERVER_PASSWORD", "")
    allow_insecure = os.getenv("ALLOW_INSECURE_SQL_ENCRYPTION", "true").lower() == "true"

    if not (host and db and user and pwd):
        raise RuntimeError("Missing SQLSERVER_* env vars for MSSQL connection.")

    parts = [
        "DRIVER={ODBC Driver 18 for SQL Server}",
        f"SERVER={host},{port}",
        f"DATABASE={db}",
        f"UID={user}",
        f"PWD={pwd}",
    ]
    if allow_insecure:
        parts.append("Encrypt=no;TrustServerCertificate=yes")
    else:
        parts.append("Encrypt=yes;TrustServerCertificate=no")
    return pyodbc.connect(";".join(parts), timeout=10, autocommit=True)

def _exec_proc_fetch_sample(cursor, sql: str, params: Tuple[Any, ...]) -> Tuple[List[str], Optional[Tuple[Any, ...]]]:
    cursor.execute(sql, params)
    row = cursor.fetchone()
    cols = [d[0] for d in cursor.description]  # names
    return cols, row

def mssql_discover(email: str) -> Dict[str, Dict[str, Tuple[str, str, bool]]]:
    """
    Return: { tool_name: { column_name: (norm_type, mssql_type, nullable_guess) } }
    nullable_guess: True if sample row shows None; otherwise True by default if row is None.
    """
    cxn = mssql_connect()
    cur = cxn.cursor()

    out: Dict[str, Dict[str, Tuple[str, str, bool]]] = {}

    # Value
    cols, row = _exec_proc_fetch_sample(
        cur,
        "EXEC dbo.SPGetAllAppsValueByUser @Email=?, @VAST=?",
        (email, None),
    )
    if cols:
        d: Dict[str, Tuple[str, str, bool]] = {}
        for i, c in enumerate(cols):
            v = row[i] if row else None
            norm = normalize_type_from_value(v)
            mss = mssql_type_from_value(v)
            nullable = (v is None) or (row is None)
            d[c] = (norm, mss, nullable)
        out["get_all_apps_value_by_user"] = d

    # Summary
    cols, row = _exec_proc_fetch_sample(
        cur,
        "EXEC dbo.SPGetAllAppsSummaryByUser @Email=?, @VAST=?, @ReportMonth=?",
        (email, None, None),
    )
    if cols:
        d = {}
        for i, c in enumerate(cols):
            v = row[i] if row else None
            norm = normalize_type_from_value(v)
            mss = mssql_type_from_value(v)
            nullable = (v is None) or (row is None)
            d[c] = (norm, mss, nullable)
        out["get_all_apps_summary_by_user"] = d

    # General
    cols, row = _exec_proc_fetch_sample(
        cur,
        "EXEC dbo.SPGetVastGeneralByUser @Email=?, @VAST=?",
        (email, None),
    )
    if cols:
        d = {}
        for i, c in enumerate(cols):
            v = row[i] if row else None
            norm = normalize_type_from_value(v)
            mss = mssql_type_from_value(v)
            nullable = (v is None) or (row is None)
            d[c] = (norm, mss, nullable)
        out["get_vast_general_by_user"] = d

    cur.close(); cxn.close()
    return out

# ----- BigQuery schema -----

def bigquery_discover() -> Dict[str, Dict[str, str]]:
    """
    Return: { tool_name: { column_name: bq_type } }
    """
    from google.cloud import bigquery

    project = os.getenv("BQ_PROJECT_ID", "")
    dataset = os.getenv("BQ_DATASET", "")
    t_value = os.getenv("BQ_TABLE_ALLAPPS_VALUE", "all_apps_values")
    t_summary = os.getenv("BQ_TABLE_ALLAPPS_SUMMARY", "all_apps_summary")
    t_general = os.getenv("BQ_TABLE_VAST_GENERAL", "vast_general")

    if not (project and dataset):
        raise RuntimeError("Missing BQ_PROJECT_ID/BQ_DATASET env vars for BigQuery.")

    client = bigquery.Client(project=project)

    def table_schema(table_name: str) -> Dict[str, str]:
        tbl = client.get_table(f"{project}.{dataset}.{table_name}")
        return {f.name: f.field_type for f in tbl.schema}

    out: Dict[str, Dict[str, str]] = {}
    try:
        out["get_all_apps_value_by_user"] = table_schema(t_value)
    except Exception:
        out["get_all_apps_value_by_user"] = {}
    try:
        out["get_all_apps_summary_by_user"] = table_schema(t_summary)
    except Exception:
        out["get_all_apps_summary_by_user"] = {}
    try:
        out["get_vast_general_by_user"] = table_schema(t_general)
    except Exception:
        out["get_vast_general_by_user"] = {}

    return out

# ----- Description generation -----

def placeholder_description(col: str, tool_name: str, norm_type: str) -> str:
    # Simple, readable placeholders. SMEs can refine later.
    human = _split_camel(col)
    scope = "current month snapshot" if CANONICAL_BY_TOOL[tool_name] == "current" else "monthly snapshot (frozen)"
    if col.lower().endswith("id"):
        return f"Identifier for {human}."
    if col.lower().endswith("count"):
        return f"Count for {human} in the {scope}."
    if any(k in col.lower() for k in ["date", "time", "at", "reportmonth"]):
        return f"Timestamp/date for {human} ({scope})."
    if any(k in col.lower() for k in ["flag", "enabled", "compliant", "is"]):
        return f"Boolean indicator for {human}."
    if any(k in col.lower() for k in ["rank", "score", "pct", "rate"]):
        return f"Numeric metric for {human} in the {scope}."
    return f"{human} ({norm_type}) in the {scope}."

def _split_camel(s: str) -> str:
    import re
    spaced = re.sub(r"(?<!^)(?=[A-Z])", " ", s)
    return spaced.replace("_", " ").strip()

def ai_descriptions_with_gemini(
    model: str,
    tool_name: str,
    cols: List[Tuple[str, str, Optional[str]]],  # (name, norm_type, bq_type?)
) -> Dict[str, str]:
    """
    Call Gemini CLI to synthesize one-sentence SME-style descriptions.
    We do NOT send real row values. Only names/types + table purpose + synthetic examples.
    """
    prompt = {
        "task": "Write one-sentence, clear, non-marketing descriptions for data columns.",
        "table": tool_name,
        "temporal_scope": CANONICAL_BY_TOOL[tool_name],
        "columns": [
            {
                "name": name,
                "normalized_type": norm,
                "bq_type": bqt or "",
                "synthetic_examples": _synthetic_examples(norm),
            }
            for (name, norm, bqt) in cols
        ],
        "style": "Short, precise, enterprise data dictionary. Avoid restating the column name.",
    }
    try:
        # Expect a 'gemini' CLI in PATH that accepts: gemini generate -m <model> -i <json>
        # If your CLI differs, adjust here.
        res = subprocess.run(
            ["gemini", "generate", "-m", model, "-i", json.dumps(prompt)],
            check=True,
            capture_output=True,
            text=True,
            timeout=60,
        )
        text = res.stdout.strip()
        # Try to parse JSON first; otherwise, assume it's newline-delimited "name: desc" pairs.
        try:
            obj = json.loads(text)
            # Expect {"columns": [{"name": "...","description": "..."}]}
            out = {c["name"]: c.get("description", "").strip() for c in obj.get("columns", []) if "name" in c}
            # Filter empties
            return {k: v for k, v in out.items() if v}
        except Exception:
            lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
            out: Dict[str, str] = {}
            for ln in lines:
                if ":" in ln:
                    k, v = ln.split(":", 1)
                    out[k.strip()] = v.strip()
            return out
    except Exception:
        # If CLI is missing or fails, fall back silently to placeholders.
        return {}

def _synthetic_examples(norm_type: str) -> List[str]:
    if norm_type == "int":
        return ["123", "42"]
    if norm_type == "decimal":
        return ["12.34", "0.95"]
    if norm_type == "datetime":
        return ["2024-02-01 00:00:00"]
    if norm_type == "bool":
        return ["true", "false"]
    return ["sample", "value"]

# ----- Merge + write -----

@dataclass
class ColMeta:
    name: str
    norm_type: str
    mssql_type: Optional[str] = None
    bq_type: Optional[str] = None
    nullable: bool = True

def merge_sources(
    mssql: Dict[str, Dict[str, Tuple[str, str, bool]]],
    bq: Dict[str, Dict[str, str]],
) -> Dict[str, Dict[str, ColMeta]]:
    """
    Combine MSSQL and BQ discoveries into unified per-tool metadata.
    """
    tools = set(mssql.keys()) | set(bq.keys())
    merged: Dict[str, Dict[str, ColMeta]] = {t: {} for t in tools}

    for tool in tools:
        m_cols = mssql.get(tool, {})
        b_cols = bq.get(tool, {})

        names = set(m_cols.keys()) | set(b_cols.keys())
        for name in names:
            if name in m_cols:
                norm, mss, nullable = m_cols[name]
            else:
                # If discovered only in BQ, infer normalized from BQ type
                bqt = b_cols.get(name, "STRING")
                norm = {
                    "STRING": "string",
                    "INT64": "int",
                    "NUMERIC": "decimal",
                    "FLOAT64": "decimal",
                    "DATETIME": "datetime",
                    "TIMESTAMP": "datetime",
                    "BOOL": "bool",
                }.get(bqt, "string")
                mss, nullable = None, True
            bqt = b_cols.get(name)
            merged[tool][name] = ColMeta(
                name=name,
                norm_type=norm,
                mssql_type=mss,
                bq_type=bqt,
                nullable=nullable,
            )
    return merged

def compute_overlaps(merged: Dict[str, Dict[str, ColMeta]]) -> Dict[str, List[str]]:
    """
    For each column name across tools, list other tools where it appears.
    """
    appearances: Dict[str, List[str]] = defaultdict(list)
    for tool, cols in merged.items():
        for name in cols:
            appearances[name].append(tool)
    also_in: Dict[str, List[str]] = {}
    for name, tools in appearances.items():
        also_in[name] = [t for t in tools]  # keep full list; we'll prune self later
    return also_in

def choose_canonical(name: str, tools_having: List[str]) -> str:
    """
    Pick a canonical source for a column given the tools that expose it.
    Heuristic is simple and stable.
    """
    lname = name.lower()
    if any(k in lname for k in ["pci", "sox", "garm", "hardware", "compliant", "ranking"]):
        if "get_vast_general_by_user" in tools_having:
            return "get_vast_general_by_user"
    if any(k in lname for k in ["vuln", "overdue", "critical", "high", "medium", "count"]):
        # prefer the snapshot table if both exist
        if "get_all_apps_value_by_user" in tools_having:
            return "get_all_apps_value_by_user"
        if "get_all_apps_summary_by_user" in tools_having:
            return "get_all_apps_summary_by_user"
    # Default preference order
    for t in ("get_vast_general_by_user", "get_all_apps_value_by_user", "get_all_apps_summary_by_user"):
        if t in tools_having:
            return t
    return tools_having[0]

def update_yaml_for_tool(
    tool_name: str,
    merged_cols: Dict[str, ColMeta],
    bq_types_for_tool: Dict[str, str],
    dry_run: bool,
    use_ai: bool,
    ai_model: Optional[str],
) -> Tuple[int, int]:
    """
    Merge into YAML:
    - Add any new fields.
    - Fill missing mssql_type/bq_type/type/nullable.
    - Add description (placeholder or AI), but never overwrite a human one.
    Returns: (added, updated)
    """
    path = TOOL_TO_FILE[tool_name]
    data = _load_yaml_or_seed(path, tool_name)

    # Index existing fields by name for quick lookups
    existing = {f["name"]: f for f in data.get("fields", []) if isinstance(f, dict) and "name" in f}

    # AI description batch input (only for fields missing description)
    ai_batch: List[Tuple[str, str, Optional[str]]] = []
    for name, meta in merged_cols.items():
        if name not in existing:
            existing[name] = {
                "name": name,
                "type": meta.norm_type,
                "mssql_type": meta.mssql_type or mssql_type_from_value(None),
                "bq_type": meta.bq_type or bq_type_from_normalized(meta.norm_type),
                "description": "",  # fill shortly
                "canonical_source": "",  # fill later
                "temporal_scope": CANONICAL_BY_TOOL[tool_name],
                "also_available_in": [],
                "nullable": bool(meta.nullable),
                "description_auto": False,
            }
        else:
            # Update only missing bits; never stomp a human description
            f = existing[name]
            f.setdefault("type", meta.norm_type)
            f.setdefault("mssql_type", meta.mssql_type or mssql_type_from_value(None))
            f.setdefault("bq_type", meta.bq_type or bq_type_from_normalized(meta.norm_type))
            f.setdefault("temporal_scope", CANONICAL_BY_TOOL[tool_name])
            f.setdefault("also_available_in", [])
            f.setdefault("nullable", bool(meta.nullable))
            f.setdefault("description_auto", False)

        # Description handling
        f = existing[name]
        if not f.get("description"):
            ai_batch.append((name, existing[name]["type"], existing[name]["bq_type"]))

    # Overlaps + canonical
    all_tools = list(TOOL_TO_FILE.keys())
    # Build reverse index by name across all tools (merged across the whole session)
    # Simple pass: look at local tool's cols and check other YAMLs too
    # (We keep it simple; a second run re-stabilizes if needed.)
    also_map: Dict[str, List[str]] = defaultdict(list)
    for other_tool in all_tools:
        if other_tool == tool_name:
            continue
        other_path = TOOL_TO_FILE[other_tool]
        other_data = _load_yaml_or_seed(other_path, other_tool)
        for f in other_data.get("fields", []):
            if isinstance(f, dict) and "name" in f:
                also_map[f["name"]].append(other_tool)

    # Fill also_available_in and canonical_source
    for name, f in existing.items():
        tools_having = [tool_name] + also_map.get(name, [])
        f["also_available_in"] = [t for t in tools_having if t != tool_name]
        f["canonical_source"] = choose_canonical(name, tools_having)

    # AI descriptions if requested
    if use_ai and ai_batch and ai_model:
        ai_desc = ai_descriptions_with_gemini(
            model=ai_model,
            tool_name=tool_name,
            cols=ai_batch,
        )
    else:
        ai_desc = {}

    # Fill descriptions (AI first, then placeholder)
    added, updated = 0, 0
    final_fields: List[Dict[str, Any]] = []
    # Preserve a stable order: by name
    for name in sorted(existing.keys(), key=str.lower):
        f = existing[name]
        if not f.get("description"):
            desc = ai_desc.get(name) or placeholder_description(
                name, tool_name, f.get("type", "string")
            )
            f["description"] = desc
            f["description_auto"] = True
            updated += 1
        final_fields.append(f)

    data["fields"] = final_fields

    if dry_run:
        # Show short diff-ish summary to stdout
        print(f"[dry-run] {tool_name}: {len(final_fields)} fields (updated {updated}) -> {path}")
    else:
        _write_yaml(path, data)
        print(f"[write]   {tool_name}: {len(final_fields)} fields (updated {updated}) -> {path}")

    return added, updated

# ----- Main -----

def main() -> None:
    ap = argparse.ArgumentParser(description="Generate/refresh field dictionaries.")
    ap.add_argument("--from", dest="sources", action="append", choices=["mssql", "bigquery"], required=True,
                    help="Which backends to read from (can be repeated).")
    ap.add_argument("--email", required=False, default=os.getenv("GEN_EMAIL", ""),
                    help="Test email to call procs with (MSSQL).")
    ap.add_argument("--write", action="store_true", help="Persist changes; default is dry run.")
    ap.add_argument("--ai", action="store_true", help="Use Gemini CLI to synthesize descriptions.")
    ap.add_argument("--model", default=os.getenv("GEMINI_MODEL", "gemini-1.5-pro"),
                    help="Gemini model name (when --ai).")
    args = ap.parse_args()

    use_mssql = "mssql" in args.sources
    use_bq = "bigquery" in args.sources

    if use_mssql and not args.email:
        print("ERROR: --email is required when using --from mssql", file=sys.stderr)
        sys.exit(2)

    # Discover schemas
    mssql_view: Dict[str, Dict[str, Tuple[str, str, bool]]] = {}
    bq_view: Dict[str, Dict[str, str]] = {}

    if use_mssql:
        try:
            mssql_view = mssql_discover(args.email)
        except Exception as exc:
            print(f"WARNING: MSSQL discovery failed: {exc}", file=sys.stderr)
            mssql_view = {}

    if use_bq:
        try:
            bq_view = bigquery_discover()
        except Exception as exc:
            print(f"WARNING: BigQuery discovery failed: {exc}", file=sys.stderr)
            bq_view = {}

    merged = merge_sources(mssql_view, bq_view)

    # Update each tool YAML
    for tool_name, cols in merged.items():
        update_yaml_for_tool(
            tool_name=tool_name,
            merged_cols=cols,
            bq_types_for_tool=bq_view.get(tool_name, {}),
            dry_run=not args.write,
            use_ai=args.ai,
            ai_model=args.model if args.ai else None,
        )

if __name__ == "__main__":
    main()
