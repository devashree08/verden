#!/usr/bin/env python3
"""
SQL Server -> BigQuery ETL (single table, full replace), with:
 - atlas descriptions
 - optional column rename (auto normalization and/or explicit map)
 - optional PK-based dedup
 - optional BQ clustering

Usage examples (PowerShell):

# VALUES table: normalize col names, partition by ReportMonth, cluster on VastID
python scripts/etl/sql_to_bq_table.py `
  --source-table dbo.AllAppsValues `
  --dest-table all_apps_values `
  --create-dataset `
  --partition `
  --cluster VastID `
  --atlas .\atlas\dbo.SPGetAllAppsValueByEID.atlas.yml

# SUMMARY table: custom SELECT, normalize, cluster by VastID,ReportMonth
python scripts/etl/sql_to_bq_table.py `
  --sql "SELECT * FROM dbo.AllAppsValuesMonthlySummary WITH (NOLOCK)" `
  --dest-table all_apps_summary `
  --partition `
  --cluster VastID,ReportMonth `
  --atlas .\atlas\dbo.SPGetAllAppsSummaryByEID.atlas.yml

# MAPPING table: explicit PK dedup and cluster on pk
python scripts/etl/sql_to_bq_table.py `
  --source-table ref.UserVastAccess `
  --dest-table sec_eid_vast_map `
  --pk EID,VastID `
  --cluster EID,VastID
"""

from __future__ import annotations

import argparse
import os
import re
import sys
import time
from typing import Iterable, Optional, List, Dict

import pandas as pd

try:
    from dotenv import load_dotenv
except Exception:
    load_dotenv = None

try:
    import yaml  # optional (for --atlas, --rename-map)
except Exception:
    yaml = None

import pyodbc
from google.cloud import bigquery


# ---------------- Env helpers ----------------

def env_get(name: str, default: Optional[str] = None) -> Optional[str]:
    val = os.getenv(name)
    return val if (val is not None and val != "") else default


# ---------------- MSSQL helpers ----------------

def connect_mssql(
    server: str,
    database: str,
    username: str,
    password: str,
    encrypt: bool = False,
    trust_server_cert: bool = True,
    timeout_sec: int = 5,
) -> pyodbc.Connection:
    driver = "{ODBC Driver 18 for SQL Server}"
    enc = "yes" if encrypt else "no"
    tsc = "yes" if trust_server_cert else "no"
    conn_str = (
        f"DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password};"
        f"Encrypt={enc};TrustServerCertificate={tsc};Connection Timeout={timeout_sec};"
    )
    return pyodbc.connect(conn_str)


def iter_dataframe_chunks(cursor: pyodbc.Cursor, sql: str, arraysize: int = 20000) -> Iterable[pd.DataFrame]:
    cursor.execute(sql)
    cols = [c[0] for c in cursor.description]
    while True:
        rows = cursor.fetchmany(arraysize)
        if not rows:
            break
        yield pd.DataFrame.from_records(rows, columns=cols)


# ---------------- Column rename / normalization ----------------

_normalize_re = re.compile(r"[^0-9A-Za-z]+")

def normalize_name(name: str) -> str:
    """Remove all non-alphanumeric chars (keeps digits). 'Tier 2 Manager' -> 'Tier2Manager'."""
    return _normalize_re.sub("", name or "")

def load_rename_map(path: Optional[str]) -> Dict[str, str]:
    if not path:
        return {}
    if yaml is None:
        print("WARN: pyyaml not installed; ignoring --rename-map.", file=sys.stderr)
        return {}
    if not os.path.exists(path):
        print(f"WARN: rename-map not found: {path}", file=sys.stderr)
        return {}
    data = yaml.safe_load(open(path, "r", encoding="utf-8")) or {}
    # expected shape: { rename: { "Tier 2 Manager": "Tier2Manager", ... } }
    remap = data.get("rename") or {}
    return {str(k): str(v) for k, v in remap.items() if isinstance(k, (str,int))}

def rename_columns(df: pd.DataFrame, mode: str, explicit: Dict[str,str]) -> None:
    """
    Apply column renames in-place:
    - explicit map first (wins)
    - then auto normalization if mode == 'auto'
    """
    current = list(df.columns)
    # explicit first
    if explicit:
        df.rename(columns=explicit, inplace=True)
    if mode == "auto":
        auto_map = {c: normalize_name(c) for c in df.columns}
        df.rename(columns=auto_map, inplace=True)
    # check for collisions (rare)
    if len(set(df.columns)) != len(df.columns):
        # resolve by suffixing duplicates
        counts: Dict[str,int] = {}
        new_cols: List[str] = []
        for c in df.columns:
            counts[c] = counts.get(c, 0) + 1
            new_cols.append(c if counts[c] == 1 else f"{c}_{counts[c]}")
        df.columns = new_cols
        print("WARN: column name collisions resolved by suffixing.", file=sys.stderr)


# ---------------- Data shaping ----------------

def normalize_report_month(df: pd.DataFrame, col: str = "ReportMonth") -> None:
    if col not in df.columns:
        return
    dt = pd.to_datetime(df[col], errors="coerce", utc=False)
    dt = pd.to_datetime(dt.dt.to_period("M").dt.start_time).dt.date
    df[col] = dt

def infer_bq_schema(df: pd.DataFrame) -> List[bigquery.SchemaField]:
    fields: List[bigquery.SchemaField] = []
    for name, dtype in df.dtypes.items():
        kind = "STRING"
        if pd.api.types.is_integer_dtype(dtype):
            kind = "INT64"
        elif pd.api.types.is_float_dtype(dtype):
            kind = "FLOAT64"
        elif pd.api.types.is_bool_dtype(dtype):
            kind = "BOOL"
        elif pd.api.types.is_datetime64_any_dtype(dtype):
            kind = "DATETIME"  # ReportMonth has been converted to DATE already
        fields.append(bigquery.SchemaField(name, kind))
    return fields


# ---------------- Atlas to BQ descriptions ----------------

def apply_atlas_descriptions(client: bigquery.Client, full_table: str, atlas_path: Optional[str]) -> None:
    if not atlas_path:
        return
    if yaml is None:
        print("WARN: pyyaml not installed; skipping atlas descriptions.", file=sys.stderr)
        return
    if not os.path.exists(atlas_path):
        print(f"WARN: atlas file not found: {atlas_path}", file=sys.stderr)
        return

    atlas = yaml.safe_load(open(atlas_path, "r", encoding="utf-8")) or {}
    cols = atlas.get("columns") or []
    desc_by_name = {
        c.get("name"): (c.get("description") or "").strip()
        for c in cols if isinstance(c, dict) and c.get("name")
    }

    table = client.get_table(full_table)
    changed = 0
    new_schema: List[bigquery.SchemaField] = []
    for f in table.schema:
        new_desc = desc_by_name.get(f.name, f.description)
        if (new_desc or "") != (f.description or ""):
            changed += 1
        new_schema.append(bigquery.SchemaField(f.name, f.field_type, mode=f.mode, description=new_desc))
    if changed:
        table.schema = new_schema
        client.update_table(table, ["schema"])
        print(f"Applied {changed} atlas descriptions to {full_table}", file=sys.stderr)


# ---------------- Main ----------------

def main() -> int:
    if load_dotenv:
        load_dotenv()

    ap = argparse.ArgumentParser("SQL Server -> BigQuery (full replace) with atlas + renames + dedup.")
    src = ap.add_mutually_exclusive_group(required=True)
    src.add_argument("--sql", help="Custom SELECT to run on MSSQL.")
    src.add_argument("--source-table", help="If set, uses SELECT * FROM <table>.")
    ap.add_argument("--dest-project", default=env_get("BQ_PROJECT", env_get("PROJECT_ID")))
    ap.add_argument("--dest-dataset", default=env_get("BQ_DATASET", env_get("DATASET_ID")))
    ap.add_argument("--dest-table", required=True, help="BigQuery table id (e.g., all_apps_values).")
    ap.add_argument("--arraysize", type=int, default=20000, help="MSSQL fetch chunk size.")
    ap.add_argument("--report-month-col", default="ReportMonth", help="Name of ReportMonth column (if any).")
    ap.add_argument("--partition", action="store_true", help="Partition on ReportMonth when creating table.")
    ap.add_argument("--cluster", help="Comma list of columns to cluster in BigQuery (e.g., 'VastID,ReportMonth').")
    ap.add_argument("--create-dataset", action="store_true", help="Create dataset if missing.")
    ap.add_argument("--atlas", help="Path to atlas *.atlas.yml (optional).")

    # NEW: column rename / normalization
    ap.add_argument("--normalize-columns", choices=["auto","none"], default="auto",
                    help="Auto: remove non-alphanumerics from all column names. None: leave as-is.")
    ap.add_argument("--rename-map", help="YAML with explicit {rename: {src: dst}} mapping (applied before auto).")

    # NEW: dedup by PK
    ap.add_argument("--pk", help="Comma list of columns that form the primary key; will drop duplicates keeping first.")

    args = ap.parse_args()

    if not args.dest_project or not args.dest_dataset:
        print("ERROR: Missing BQ project/dataset. Set BQ_PROJECT/BQ_DATASET or pass --dest-*.", file=sys.stderr)
        return 2
    full_table = f"{args.dest_project}.{args.dest_dataset}.{args.dest_table}"

    sql = args.sql or f"SELECT * FROM {args.source_table}"

    # MSSQL env
    server = env_get("DB_SERVER")
    database = env_get("DB_DATABASE")
    username = env_get("DB_USERNAME")
    password = env_get("DB_PASSWORD")
    if not all([server, database, username, password]):
        print("ERROR: Missing MSSQL env vars: DB_SERVER, DB_DATABASE, DB_USERNAME, DB_PASSWORD", file=sys.stderr)
        return 2
    encrypt = env_get("DB_ENCRYPT", "false").lower() == "true"
    trust = env_get("DB_TRUST_SERVER_CERT", "true").lower() == "true"
    conn_timeout = int(env_get("DB_CONN_TIMEOUT_SEC", "5"))

    t0 = time.time()
    print(f"ETL start: MSSQL -> {full_table}", file=sys.stderr)

    # Read from MSSQL
    with connect_mssql(server, database, username, password, encrypt, trust, conn_timeout) as conn:
        cur = conn.cursor()
        dfs = []
        total = 0
        for i, chunk in enumerate(iter_dataframe_chunks(cur, sql, arraysize=args.arraysize), start=1):
            # Column rename/normalize BEFORE report month conversion (so we know final column names)
            explicit_map = load_rename_map(args.rename_map)
            rename_columns(chunk, args.normalize_columns, explicit_map)
            normalize_report_month(chunk, args.report_month_col)
            total += len(chunk.index)
            dfs.append(chunk)
            print(f" - fetched chunk {i}: {len(chunk.index)} rows", file=sys.stderr)

    df = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()
    print(f"Fetched rows: {total}", file=sys.stderr)

    # Optional PK-based dedup
    if args.pk:
        pk_cols = [c.strip() for c in args.pk.split(",") if c.strip()]
        missing = [c for c in pk_cols if c not in df.columns]
        if missing:
            print(f"ERROR: --pk columns not found in DataFrame: {missing}", file=sys.stderr)
            return 2
        before = len(df)
        df.drop_duplicates(subset=pk_cols, keep="first", inplace=True, ignore_index=True)
        after = len(df)
        if after != before:
            print(f"Deduped by PK {pk_cols}: {before}->{after} rows", file=sys.stderr)

    # BQ client
    bq = bigquery.Client(project=args.dest_project)

    # Ensure dataset
    if args.create_dataset:
        ds_ref = bigquery.Dataset(f"{args.dest_project}.{args.dest_dataset}")
        try:
            bq.get_dataset(ds_ref)
        except Exception:
            ds_ref.location = env_get("BQ_LOCATION", "US")
            bq.create_dataset(ds_ref, exists_ok=True)
            print(f"Dataset ensured: {args.dest_project}.{args.dest_dataset}", file=sys.stderr)

    # Prepare load
    schema = infer_bq_schema(df)
    job_config = bigquery.LoadJobConfig(
        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
        schema=schema,
    )
    # Partitioning by ReportMonth (if present)
    if args.partition and args.report_month_col in df.columns:
        job_config.time_partitioning = bigquery.TimePartitioning(
            type_=bigquery.TimePartitioningType.DAY,
            field=args.report_month_col,
        )
    # Clustering
    if args.cluster:
        cluster_fields = [c.strip() for c in args.cluster.split(",") if c.strip()]
        job_config.clustering_fields = cluster_fields

    # Load
    job = bq.load_table_from_dataframe(df, full_table, job_config=job_config)
    job.result()

    tbl = bq.get_table(full_table)
    print(
        f"Loaded {tbl.num_rows} rows into {full_table} (fields={len(tbl.schema)}) in {round(time.time()-t0, 2)}s",
        file=sys.stderr,
    )

    # Atlas descriptions
    apply_atlas_descriptions(bq, full_table, args.atlas)

    print("ETL complete.", file=sys.stderr)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
