# MCP Enterprise Template (HTTP on Cloud Run)

Small, readable **Model Context Protocol (MCP) HTTP server** you can clone for new services.
- **Primary data**: On-prem **MS SQL Server** (via ODBC 18)
- **Fallback**: **BigQuery** snapshot tables
- **Auth**: Keycloak OIDC (validate JWT, use `email` claim)
- **Transport**: MCP over **HTTP** (works with MCP Inspector)
- **Tooling**: Python 3.13.7, FastMCP 2.12.3, uv, ruff
- **Runtime**: Dockerfile → Artifact Registry → Cloud Run

> Goal (Phase-1): run this MCP server on Cloud Run. Keep code simple, boring, and enterprise-ready.

---

## How it works (end-to-end)

- **Single endpoint**: MCP over HTTP uses **one path** (e.g., `/mcp/`). Tools are *not* separate REST routes.
- **Discovery → tool selection**: Clients (e.g., MCP Inspector, agents) hit `/mcp/`, read tool **descriptions** and **JSON Schemas**, then call a tool by name on the same endpoint.
- **Auth**: Requests include `Authorization: Bearer <JWT>` (Keycloak). Server verifies `iss/aud/exp/nbf`, extracts `email`, and **requires** the input `email` equals the token’s `email` (Phase-1 policy).
- **Data access** (adapter switch):
  - `DB_BACKEND=mssql` → run stored procs via **pyodbc + ODBC Driver 18**.
  - `DB_BACKEND=bigquery` → run SELECTs on exported BQ tables with the same filters.
- **Output shape** (all tools):  
  ```json
  { "rows": [ { /* columns (300–400) */ } ] }
  ```
- **Field dictionaries**: `/resources/describe_fields/<tool>` serves a YAML/JSON list of columns/types/descriptions so clients/devs understand large schemas without bloating tool I/O.

---

## Tools (1 file = 1 tool)

1) **get_all_apps_value_by_user** → `dbo.SPGetAllAppsValueByUser`  
   Use for **current month (daily snapshot)**.
   - Inputs: `email` (required), `vast` (CSV or array), `limit` (default 1000, max 15000), `offset` (default 0)

2) **get_all_apps_summary_by_user** → `dbo.SPGetAllAppsSummaryByUser`  
   Use for **historical monthly snapshots** (e.g., “Feb 2024”).
   - Inputs: `email`, `vast`, `report_month` (accepts `YYYY-MM` or NL like “February 2024”; normalized to `YYYY-MM-01 00:00:00`), `limit`, `offset`

3) **get_vast_general_by_user** → `dbo.SPGetVastGeneralByUser`  
   Use for **compliance/general** (includes decommissioned).
   - Inputs: `email`, `vast`, `limit`, `offset`

**Shared normalization**:
- `vast`: accept CSV **or** array; normalize internally to CSV (empty ⇒ omit filter).
- `report_month`: parse NL or `YYYY-MM` → literal `YYYY-MM-01 00:00:00` (no timezone math).

---

## Repo structure (stable)

```text
framework/   # Reusable engine: bootstrap, auth, adapters, schemas
  core/      # HTTP startup, tool/resource registration, config, logging
  auth/      # OIDC token validation (discovery -> JWKS), extracts `email`
  adapters/  # mssql (primary), bigquery (fallback)
  schemas/   # Shared JSON Schemas for inputs/outputs
server/      # Project layer: add/change code here
  tools/     # 3 SQL tools + sample tool (one file per tool)
  resources/ # describe_fields resource + static field dictionaries
  prompts/   # optional prompt files (md/txt) for agents
  config/    # runtime env README (no secrets)
scripts/     # one-offs (e.g., generate field dictionaries)
ops/         # Dockerfile + deploy notes (Cloud Run)
tests/       # smoke tests/checklists (auth + tool contracts)
.env.example # local placeholders only (no secrets)
pyproject.toml       # uv-managed deps (pins FastMCP 2.12.3, etc.)
ruff.toml            # lint + docstring rules
README.md            # this file
```

**Ownership**:
- `framework/` is tiny and reusable across projects (plumbing).
- `server/` is where you spend time (tools/resources/prompts).
- `ops/` holds the **single** Dockerfile and Cloud Run notes.

---

## Tooling

- **Python** 3.13.7
- **uv** for env/deps (`uv sync` creates/uses a venv; no separate `python -m venv`)
- **ruff** for lint + docstrings
- **MCP Inspector** for local testing (HTTP/SSE)
- **Docker** to containerize for Cloud Run
- Later in CI/CD: **Cloud Build**, **Artifact Registry**, **Cloud Run** (org policy may enforce CMEK)

---

## Local dev (Windows / VS Code)

> You can create files & lint now. Running the server comes in the next step after we add the entrypoint.

```bash
# Install deps (creates .venv under the repo)
uv sync

# Lint
uv run ruff check .
```

When the entrypoint is added:
```bash
# Start the MCP HTTP server (exposes /mcp/)
# uv run python -m framework.core.bootstrap

# Then point MCP Inspector to: http://localhost:8080/mcp/
```

> If `uv init` created a `src/` directory, delete it. We’re building an app, not a pip package, and we’ve standardized the structure above.

---

## Configuration (env + secrets)

- **Non-secret env** (examples):  
  `OIDC_ISSUER_URI`, `OIDC_ALLOWED_AUDIENCE`, `DB_BACKEND`, `ENCRYPT=false`, `TRUST_SERVER_CERTIFICATE=true`.
- **Secrets** (examples):  
  `SQLSERVER_HOST`, `SQLSERVER_DATABASE`, `SQLSERVER_USERNAME`, `SQLSERVER_PASSWORD`.

**Cloud Run** maps Secret Manager → **env vars** at deploy (e.g., `--set-secrets=SQLSERVER_PASSWORD=...:latest`).  
The app **does not call Secret Manager** directly; it just reads env vars.

---

## GCP deployment (overview)

1) **Build** container from `ops/Dockerfile` (Cloud Build or local) → push to **Artifact Registry**.  
2) **Deploy** to **Cloud Run** with:
   - **Networking**: **Direct VPC egress** (preferred) `--network --subnet --vpc-egress=all-traffic`, or **Serverless VPC Access** connector if required.
   - **Secrets**: `--set-secrets=...` maps SQL creds to env vars.
   - **Env**: `--set-env-vars=OIDC_ISSUER_URI=...,OIDC_ALLOWED_AUDIENCE=...,DB_BACKEND=mssql` etc.
   - **Auth**: service enforces `Authorization: Bearer <JWT>` on all tool/resource calls.
   - **CMEK**: Your org may require `--kms-key="projects/.../cryptoKeys/..."` for Cloud Run/AR/SM.

**Service config (cost-friendly defaults)**: 1 vCPU, 512Mi, concurrency 20, timeout 300s, min instances 0.

---

## Adding a new tool (quick recipe)

1) Copy `server/tools/sample_echo_tool.py` → rename file + function.  
2) Import shared JSON Schemas from `framework/schemas`.  
3) Keep logic tiny: **auth → normalize → adapter call → return `{ "rows": [...] }`**.  
4) Add/extend a field dictionary YAML in `server/resources/fields/` if the result has many columns.  
5) Test in MCP Inspector.

---

## Python dependencies (pinned)

- `fastmcp==2.12.3` — MCP server (HTTP)
- `pyodbc==5.1.0` — SQL Server ODBC 18
- `pyjwt[crypto]==2.9.0` — JWT validation
- `requests==2.32.3` — OIDC discovery (well-known → JWKS)
- `pydantic==2.8.2` — light validation
- `python-dateutil==2.9.0.post0` — parse “Feb 2024” → `YYYY-MM-01 00:00:00`
- `ruamel.yaml==0.18.6` — field dictionaries
- `google-cloud-bigquery==3.25.0` — fallback adapter

---

## Testing

- **MCP Inspector**: end-to-end tool checks over HTTP/SSE.
- **pytest** (later): smoke tests for auth policy and tool contract shape.

---

## Conventions

- **1 file = 1 tool**. No SQL in tools; tools call adapters.
- **Unified output**: always `{ "rows": [ … ] }`.
- **Input normalization**: `vast` CSV/array → CSV, `report_month` → `YYYY-MM-01 00:00:00`.
- **Pagination**: `limit` (default 1000, max 15000), `offset` (default 0).
- **Logging**: JSON; never log tokens; keep PII minimal (email only if needed for audit).
- **Comments**: module header docstrings + function docstrings (**Args/Returns/Raises**). Inline comments explain **why**, not what.
