import argparse
import logging
import pyodbc
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions

# ---------------------------------------------------------------------
# 1. Define the custom step to read from SQL Server
# ---------------------------------------------------------------------
class ReadFromSqlServerFn(beam.DoFn):
    """
    A custom DoFn to read data from an on-prem SQL Server.
    """
    def __init__(self, server, database, username, password, driver):
        self.server = server
        self.database = database
        self.username = username
        self.password = password
        self.driver = driver
        self.conn_str = (
            f"DRIVER={{{self.driver}}};"
            f"SERVER={self.server};"
            f"DATABASE={self.database};"
            f"UID={self.username};"
            f"PWD={self.password};"
        )
        
    def setup(self):
        """
        Runs once per worker. This is the place to 'import' and
        create non-serializable clients like a DB connection.
        """
        import pyodbc
        self.connection = pyodbc.connect(self.conn_str)

    def process(self, query):
        """
        Runs for each input element. The input 'query' is just a string.
        We yield (output) one row at a time.
        """
        try:
            logging.info(f"Executing query: {query}")
            cursor = self.connection.cursor()
            cursor.execute(query)
            
            # Get column names from the cursor description
            columns = [column[0] for column in cursor.description]
            
            for row in cursor.fetchall():
                # Yield each row as a dictionary
                yield dict(zip(columns, row))
                
        except Exception as e:
            logging.error(f"Error executing query: {e}")
            raise
        
    def teardown(self):
        """
S        Runs once per worker when it's shutting down.
        """
        if hasattr(self, 'connection'):
            self.connection.close()

# ---------------------------------------------------------------------
# 2. Define the pipeline arguments (for reusability)
# ---------------------------------------------------------------------
class SqlPipelineOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        parser.add_argument("--input_query", required=True, help="SQL query to run on the source database")
        parser.add_argument("--output_table", required=True, help="BigQuery table spec (project:dataset.table)")
        
        # SQL Server connection parameters
        parser.add_argument("--sql_server", required=True)
        parser.add_argument("--sql_database", required=True)
        parser.add_argument("--sql_username", required=True)
        parser.add_argument("--sql_password", required=True)
        parser.add_argument("--sql_driver", default="ODBC Driver 17 for SQL Server")

# ---------------------------------------------------------------------
# 3. Define and run the main pipeline
# ---------------------------------------------------------------------
def run():
    # Parse all pipeline arguments
    pipeline_options = PipelineOptions()
    sql_options = pipeline_options.view_as(SqlPipelineOptions)
    
    # This is the BigQuery schema.
    # For a robust pipeline, you'd auto-detect this or pass it.
    # For now, let's hardcode a simple example.
    # OR, we can let BigQuery auto-detect from the dictionaries!
    # We will use WRITE_TRUNCATE (overwrites) and auto-create.
    
    with beam.Pipeline(options=pipeline_options) as p:
        
        (   p
            # Start the pipeline with the single query string
            | 'Create Query' >> beam.Create([sql_options.input_query])
            
            # Run the custom Read DoFn. This will run on Dataflow workers
            # inside your VPC and connect to your on-prem server.
            | 'Read from SQL Server' >> beam.ParDo(
                ReadFromSqlServerFn(
                    server=sql_options.sql_server,
                    database=sql_options.sql_database,
                    username=sql_options.sql_username,
                    password=sql_options.sql_password,
                    driver=sql_options.sql_driver
                )
            )
            
            # Write the dictionaries directly to BigQuery
            | 'Write to BigQuery' >> beam.io.WriteToBigQuery(
                sql_options.output_table,
                schema="SCHEMA_AUTODETECT",
                create_disposition=beam.io.BigQueryDisposition.CREATE_NEVER,
                write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE
            )
        )

if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    run()
